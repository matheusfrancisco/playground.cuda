{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a `qdrep` report file which can be used in a variety of manners. We use the `--stats=true` flag here to indicate we would like summary statistics printed. There is quite a lot of information printed:\n",
    "\n",
    "- Profile configuration details\n",
    "- Report file(s) generation details\n",
    "- **CUDA API Statistics**\n",
    "- **CUDA Kernel Statistics**\n",
    "- **CUDA Memory Operation Statistics (time and size)**\n",
    "- OS Runtime API Statistics\n",
    "\n",
    "In this lab you will primarily be using the 3 sections in **bold** above. In the next lab, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `CUDA Kernel Statistics` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-828a-ad31-1e36-6817.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-828a-ad31-1e36-6817.qdrep\"\n",
      "Exporting 4607 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-828a-ad31-1e36-6817.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   90.2      2186375371           1    2186375371.0      2186375371      2186375371  cudaDeviceSynchronize                                                           \n",
      "    8.9       216279016           3      72093005.3           37076       216115416  cudaMallocManaged                                                               \n",
      "    0.9        20735244           3       6911748.0         6142023         8106337  cudaFree                                                                        \n",
      "    0.0           79387           1         79387.0           79387           79387  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0      2186368909           1    2186368909.0      2186368909      2186368909  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.5        68485248        2304         29724.5            1888          182016  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.5        21025216         768         27376.6            1152          167840  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   59.1      5029538603         260      19344379.2           19359       100134778  poll                                                                            \n",
      "   39.7      3374162296         259      13027653.7            9112       100097855  sem_timedwait                                                                   \n",
      "    0.9        74166417         655        113231.2            1051        17310663  ioctl                                                                           \n",
      "    0.3        22874586          91        251369.1            1383         8037722  mmap                                                                            \n",
      "    0.0         1779120          77         23105.5            4750           46198  open64                                                                          \n",
      "    0.0          187499           4         46874.7           32150           53083  pthread_create                                                                  \n",
      "    0.0          166431           3         55477.0           52719           60138  fgets                                                                           \n",
      "    0.0          154973          25          6198.9            1519           26738  fopen                                                                           \n",
      "    0.0           91795          11          8345.0            3708           14160  write                                                                           \n",
      "    0.0           66202          14          4728.7            1336            7770  munmap                                                                          \n",
      "    0.0           41031           5          8206.2            3625           10794  open                                                                            \n",
      "    0.0           28366          18          1575.9            1042            5163  fclose                                                                          \n",
      "    0.0           22917          17          1348.1            1001            4513  fcntl                                                                           \n",
      "    0.0           21655          11          1968.6            1130            3220  read                                                                            \n",
      "    0.0           18393           2          9196.5            7583           10810  socket                                                                          \n",
      "    0.0           18375           2          9187.5            1015           17360  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           12184           1         12184.0           12184           12184  sem_wait                                                                        \n",
      "    0.0            9836           3          3278.7            1576            4497  fread                                                                           \n",
      "    0.0            9192           4          2298.0            1988            2528  mprotect                                                                        \n",
      "    0.0            8564           1          8564.0            8564            8564  connect                                                                         \n",
      "    0.0            7531           1          7531.0            7531            7531  pipe2                                                                           \n",
      "    0.0            2562           1          2562.0            2562            2562  bind                                                                            \n",
      "    0.0            1967           1          1967.0            1967            1967  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report5.qdrep\"\n",
      "Report file moved to \"/dli/task/report5.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-6356-bf8a-0520-be1d.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-6356-bf8a-0520-be1d.qdrep\"\n",
      "Exporting 4239 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-6356-bf8a-0520-be1d.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   56.3       223762950           3      74587650.0           23586       223690655  cudaMallocManaged                                                               \n",
      "   38.2       151899325           1     151899325.0       151899325       151899325  cudaDeviceSynchronize                                                           \n",
      "    5.4        21436979           3       7145659.7         6520990         8181398  cudaFree                                                                        \n",
      "    0.0           66915           1         66915.0           66915           66915  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       151887114           1     151887114.0       151887114       151887114  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.6        68872768        2304         29892.7            1888          185408  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.4        21017216         768         27366.2            1184          167456  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   54.1      1370530591          74      18520683.7           23185       100143994  poll                                                                            \n",
      "   41.7      1055002120          74      14256785.4           15234       100075216  sem_timedwait                                                                   \n",
      "    3.1        79241089         653        121349.3            1086        17726517  ioctl                                                                           \n",
      "    0.9        23736155          91        260836.9            1619         8059679  mmap                                                                            \n",
      "    0.1         1658550          77         21539.6            6033           52281  open64                                                                          \n",
      "    0.0          197743           4         49435.7           32874           61990  pthread_create                                                                  \n",
      "    0.0          186929           3         62309.7           52700           70186  fgets                                                                           \n",
      "    0.0          155568          25          6222.7            1582           27059  fopen                                                                           \n",
      "    0.0           95714          11          8701.3            4270           14600  write                                                                           \n",
      "    0.0           75585          15          5039.0            1800            8109  munmap                                                                          \n",
      "    0.0           56455           5         11291.0            6778           15427  open                                                                            \n",
      "    0.0           34371          18          1909.5            1141            5453  fclose                                                                          \n",
      "    0.0           28760           3          9586.7            1047           17303  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           25952          13          1996.3            1058            2841  read                                                                            \n",
      "    0.0           24082          17          1416.6            1005            5336  fcntl                                                                           \n",
      "    0.0           19357           1         19357.0           19357           19357  sem_wait                                                                        \n",
      "    0.0           19326           1         19326.0           19326           19326  bind                                                                            \n",
      "    0.0           18990           2          9495.0            8479           10511  socket                                                                          \n",
      "    0.0           11045           1         11045.0           11045           11045  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0           11044           3          3681.3            1756            5389  fread                                                                           \n",
      "    0.0            9989           4          2497.3            1980            3051  mprotect                                                                        \n",
      "    0.0            9157           1          9157.0            9157            9157  connect                                                                         \n",
      "    0.0            7924           1          7924.0            7924            7924  pipe2                                                                           \n",
      "    0.0            2256           1          2256.0            2256            2256  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report10.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report10.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-479c-df13-adf6-16e4.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-479c-df13-adf6-16e4.qdrep\"\n",
      "Exporting 3304 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-479c-df13-adf6-16e4.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   92.8       293912758           1     293912758.0       293912758       293912758  cudaDeviceSynchronize                                                           \n",
      "    6.6        20980524           3       6993508.0         6402609         8095782  cudaFree                                                                        \n",
      "    0.6         1966480           1       1966480.0         1966480         1966480  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       293910891           1     293910891.0       293910891       293910891  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.7        69024512        2304         29958.6            1888          182048  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.3        20965440         768         27298.8            1152          165792  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   60.8      1366114629          73      18713899.0        10034052       100149873  poll                                                                            \n",
      "   38.3       859085277          75      11454470.4          207874       103976635  sem_timedwait                                                                   \n",
      "    0.9        20605980           3       6868660.0         6266774         8017697  mmap                                                                            \n",
      "    0.0           55131          14          3937.9            1292           22749  ioctl                                                                           \n",
      "    0.0           16234           3          5411.3            4770            6664  munmap                                                                          \n",
      "    0.0            6687           1          6687.0            6687            6687  pthread_rwlock_timedrdlock                                                      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report11.qdrep\"\n",
      "Report file moved to \"/dli/task/report11.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](../edit/04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\r\n",
      "Number of SMs: 40\r\n",
      "Compute Capability Major: 7\r\n",
      "Compute Capability Minor: 5\r\n",
      "Warp Size: 32\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-vector-add/01-vector-add.cu(83): warning: variable \"computeCapabilityMajor\" was declared but never referenced\n",
      "\n",
      "01-vector-add/01-vector-add.cu(84): warning: variable \"computeCapabilityMinor\" was declared but never referenced\n",
      "\n",
      "01-vector-add/01-vector-add.cu(85): warning: variable \"multiProcessorCount\" was declared but never referenced\n",
      "\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-08d9-ea95-6fd3-dc7b.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-08d9-ea95-6fd3-dc7b.qdrep\"\n",
      "Exporting 4252 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-08d9-ea95-6fd3-dc7b.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   55.2       224556152           3      74852050.7           18399       224463807  cudaMallocManaged                                                               \n",
      "   39.5       160807447           1     160807447.0       160807447       160807447  cudaDeviceSynchronize                                                           \n",
      "    5.2        21323447           3       7107815.7         6450343         8243524  cudaFree                                                                        \n",
      "    0.0           60012           1         60012.0           60012           60012  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       160795378           1     160795378.0       160795378       160795378  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.5        69153216        2304         30014.4            2144          186272  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.5        21265504         768         27689.5            1472          167872  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   54.0      1364792605          75      18197234.7           24258       100153311  poll                                                                            \n",
      "   41.8      1057871118          75      14104948.2           22393       100080425  sem_timedwait                                                                   \n",
      "    3.1        78880703         665        118617.6            1099        17978679  ioctl                                                                           \n",
      "    0.9        23696270          91        260398.6            1411         8147462  mmap                                                                            \n",
      "    0.1         1876638          77         24371.9            4804           44507  open64                                                                          \n",
      "    0.0          217251           4         54312.8           39788           68768  pthread_create                                                                  \n",
      "    0.0          162691           3         54230.3           51845           58458  fgets                                                                           \n",
      "    0.0          142022          25          5680.9            1559           23592  fopen                                                                           \n",
      "    0.0          105499          11          9590.8            3883           19700  write                                                                           \n",
      "    0.0           75113          15          5007.5            1850            8801  munmap                                                                          \n",
      "    0.0           61097           4         15274.2            1057           29713  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           41112           5          8222.4            2984           12711  open                                                                            \n",
      "    0.0           32447          13          2495.9            1482            4393  read                                                                            \n",
      "    0.0           28391          17          1670.1            1068            5634  fclose                                                                          \n",
      "    0.0           24035          17          1413.8            1001            5326  fcntl                                                                           \n",
      "    0.0           20164           2         10082.0            9520           10644  socket                                                                          \n",
      "    0.0           11539           3          3846.3            1776            5541  fread                                                                           \n",
      "    0.0            9773           4          2443.3            1987            3212  mprotect                                                                        \n",
      "    0.0            8694           1          8694.0            8694            8694  connect                                                                         \n",
      "    0.0            8592           1          8592.0            8592            8592  pipe2                                                                           \n",
      "    0.0            8523           1          8523.0            8523            8523  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            3094           1          3094.0            3094            3094  bind                                                                            \n",
      "    0.0            2622           1          2622.0            2622            2622  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report12.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report12.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-dd40-1019-5303-d6c2.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-dd40-1019-5303-d6c2.qdrep\"\n",
      "Exporting 1034 events: [==================================================100%]85%         ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-dd40-1019-5303-d6c2.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0       287939088           1     287939088.0       287939088       287939088  cudaMallocManaged                                                               \n",
      "    0.0           93854           1         93854.0           93854           93854  cudaFree                                                                        \n",
      "\n",
      "\n",
      "\n",
      "CUDA trace data was not collected.\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   50.6       349955616          17      20585624.5           13418       100075262  sem_timedwait                                                                   \n",
      "   33.3       230610004          16      14413125.3           24285        86114203  poll                                                                            \n",
      "   15.3       106070549         642        165218.9            1033        17430962  ioctl                                                                           \n",
      "    0.4         2823855          85         33221.8            1495          841788  mmap                                                                            \n",
      "    0.2         1710674          77         22216.5            5443           46039  open64                                                                          \n",
      "    0.0          186260           4         46565.0           34137           53583  pthread_create                                                                  \n",
      "    0.0          163619           3         54539.7           51654           58639  fgets                                                                           \n",
      "    0.0          137031          25          5481.2            1461           26574  fopen                                                                           \n",
      "    0.0          114586          11         10416.9            4031           17660  write                                                                           \n",
      "    0.0          106389           7         15198.4            7668           27738  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           44920          10          4492.0            1315            9124  munmap                                                                          \n",
      "    0.0           42223          29          1456.0            1014            5929  fcntl                                                                           \n",
      "    0.0           41200           5          8240.0            4066           12676  open                                                                            \n",
      "    0.0           32152          13          2473.2            1284            4520  read                                                                            \n",
      "    0.0           29710          18          1650.6            1080            6074  fclose                                                                          \n",
      "    0.0           22194           2         11097.0            9567           12627  fread                                                                           \n",
      "    0.0           18277           2          9138.5            7088           11189  socket                                                                          \n",
      "    0.0           11623           1         11623.0           11623           11623  pipe2                                                                           \n",
      "    0.0           11530           4          2882.5            1945            5087  mprotect                                                                        \n",
      "    0.0            9825           1          9825.0            9825            9825  connect                                                                         \n",
      "    0.0            2704           1          2704.0            2704            2704  bind                                                                            \n",
      "    0.0            1774           1          1774.0            1774            1774  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report13.qdrep\"\n",
      "Report file moved to \"/dli/task/report13.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-4859-f7fa-273d-087c.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-4859-f7fa-273d-087c.qdrep\"\n",
      "Exporting 4275 events: [==================================================100%][===============================================94%    ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-4859-f7fa-273d-087c.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   60.4       251699910           3      83899970.0           30247       251556669  cudaMallocManaged                                                               \n",
      "   34.4       143300327           1     143300327.0       143300327       143300327  cudaDeviceSynchronize                                                           \n",
      "    5.1        21398500           3       7132833.3         6445986         8326956  cudaFree                                                                        \n",
      "    0.0           63251           1         63251.0           63251           63251  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       143336037           1     143336037.0       143336037       143336037  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.5        68971488        2304         29935.5            2208          171392  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.5        21153792         768         27544.0            1440          167776  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   54.0      1517890150          81      18739384.6           21720       100157486  poll                                                                            \n",
      "   41.4      1164232580          81      14373241.7           11898       100075693  sem_timedwait                                                                   \n",
      "    3.7       103208610         674        153128.5            1014        17558750  ioctl                                                                           \n",
      "    0.8        23816372          91        261718.4            1589         8241994  mmap                                                                            \n",
      "    0.1         1633066          77         21208.6            7455           45594  open64                                                                          \n",
      "    0.0          228763           3         76254.3           71180           78822  fgets                                                                           \n",
      "    0.0          221446           4         55361.5           50578           59026  pthread_create                                                                  \n",
      "    0.0          173835          25          6953.4            1897           26884  fopen                                                                           \n",
      "    0.0           96805          11          8800.5            4445           16364  write                                                                           \n",
      "    0.0           72421          14          5172.9            1388            9498  munmap                                                                          \n",
      "    0.0           48672           5          9734.4            5555           13886  open                                                                            \n",
      "    0.0           37603          18          2089.1            1313            5084  fclose                                                                          \n",
      "    0.0           34694          24          1445.6            1004            5054  fcntl                                                                           \n",
      "    0.0           34328           3         11442.7            1193           22254  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           20416           9          2268.4            1732            3504  read                                                                            \n",
      "    0.0           20228           2         10114.0            8208           12020  socket                                                                          \n",
      "    0.0           12345           4          3086.2            1985            3876  mprotect                                                                        \n",
      "    0.0           11595           3          3865.0            1998            4883  fread                                                                           \n",
      "    0.0           10384           1         10384.0           10384           10384  connect                                                                         \n",
      "    0.0            9043           1          9043.0            9043            9043  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            7551           1          7551.0            7551            7551  pipe2                                                                           \n",
      "    0.0            2764           1          2764.0            2764            2764  bind                                                                            \n",
      "    0.0            1692           1          1692.0            1692            1692  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report15.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report15.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-dbec-1c47-2cf7-f765.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-dbec-1c47-2cf7-f765.qdrep\"\n",
      "Exporting 1926 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-dbec-1c47-2cf7-f765.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   83.9       398560164           3     132853388.0           36156       398416923  cudaMallocManaged                                                               \n",
      "   11.7        55661312           1      55661312.0        55661312        55661312  cudaDeviceSynchronize                                                           \n",
      "    4.3        20512173           3       6837391.0         6011978         8356480  cudaFree                                                                        \n",
      "    0.0          214885           4         53721.3           13033          168629  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "   96.9        54065731           3      18021910.3        16696821        20139045  initWith(float, float*, int)                                                                                                                                                                                                                                                                                                                 \n",
      "    3.1         1714711           1       1714711.0         1714711         1714711  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21228192         768         27640.9            1632          160608  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   47.6       829452566          39      21268014.5           41324       100130103  poll                                                                            \n",
      "   43.7       761566786          34      22399023.1           13386       100072564  sem_timedwait                                                                   \n",
      "    7.1       123153204         662        186032.0            1010        18378529  ioctl                                                                           \n",
      "    1.4        25265716          91        277645.2            1564         8276203  mmap                                                                            \n",
      "    0.2         2881214          77         37418.4            4727           71040  open64                                                                          \n",
      "    0.0          282017          11         25637.9            8192           66660  write                                                                           \n",
      "    0.0          204611           4         51152.8           35822           66659  pthread_create                                                                  \n",
      "    0.0          168519           3         56173.0           53352           61333  fgets                                                                           \n",
      "    0.0          158534          25          6341.4            1546           34474  fopen                                                                           \n",
      "    0.0          126867          69          1838.7            1006            9444  fcntl                                                                           \n",
      "    0.0           93568          14          6683.4            2869           13924  munmap                                                                          \n",
      "    0.0           48212           3         16070.7            4213           29309  fread                                                                           \n",
      "    0.0           42143           5          8428.6            3400           14074  open                                                                            \n",
      "    0.0           33057          18          1836.5            1061            8272  fclose                                                                          \n",
      "    0.0           26869          13          2066.8            1083            3634  read                                                                            \n",
      "    0.0           22037           4          5509.2            1107           11194  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           19561           2          9780.5            8552           11009  socket                                                                          \n",
      "    0.0           14466           1         14466.0           14466           14466  pipe2                                                                           \n",
      "    0.0           10145           1         10145.0           10145           10145  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            9261           4          2315.2            2033            2664  mprotect                                                                        \n",
      "    0.0            8589           1          8589.0            8589            8589  connect                                                                         \n",
      "    0.0            3159           1          3159.0            3159            3159  bind                                                                            \n",
      "    0.0            1744           1          1744.0            1744            1744  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report16.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report16.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](../edit/08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-a57f-92f3-cdf3-ac2e.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-a57f-92f3-cdf3-ac2e.qdrep\"\n",
      "Exporting 1891 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-a57f-92f3-cdf3-ac2e.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   75.5       231811208           3      77270402.7           18323       231710208  cudaMallocManaged                                                               \n",
      "   17.2        52896541           1      52896541.0        52896541        52896541  cudaDeviceSynchronize                                                           \n",
      "    7.2        21964940           3       7321646.7         6264380         9318731  cudaFree                                                                        \n",
      "    0.1          229558           1        229558.0          229558          229558  cudaMemPrefetchAsync                                                            \n",
      "    0.0           72760           4         18190.0            5769           51480  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "   96.8        51189317           3      17063105.7        15386058        20121688  initWith(float, float*, int)                                                                                                                                                                                                                                                                                                                 \n",
      "    3.2         1711287           1       1711287.0         1711287         1711287  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21388288         768         27849.3            1632          179104  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   50.8       763155104          41      18613539.1           22898       106524077  poll                                                                            \n",
      "   42.1       632454531          40      15811363.3           20901       102845734  sem_timedwait                                                                   \n",
      "    5.3        80359788         657        122313.2            1005        17693485  ioctl                                                                           \n",
      "    1.6        24499843          91        269229.0            1490         9235026  mmap                                                                            \n",
      "    0.1         1909444          77         24798.0            4946           42415  open64                                                                          \n",
      "    0.0          200655           4         50163.8           34737           60151  pthread_create                                                                  \n",
      "    0.0          173202           3         57734.0           52270           61747  fgets                                                                           \n",
      "    0.0          137334          25          5493.4            1496           26656  fopen                                                                           \n",
      "    0.0          100898          11          9172.5            2705           16447  write                                                                           \n",
      "    0.0           86798          16          5424.9            1756           17678  munmap                                                                          \n",
      "    0.0           58006           5         11601.2            3084           28401  open                                                                            \n",
      "    0.0           39836          30          1327.9            1000            5610  fcntl                                                                           \n",
      "    0.0           34341          13          2641.6            1370            5461  read                                                                            \n",
      "    0.0           29129          18          1618.3            1108            4342  fclose                                                                          \n",
      "    0.0           22808           2         11404.0           11333           11475  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           21853           2         10926.5           10915           10938  socket                                                                          \n",
      "    0.0           11632           3          3877.3            2610            5116  fread                                                                           \n",
      "    0.0           11386           4          2846.5            2437            3680  mprotect                                                                        \n",
      "    0.0            9303           1          9303.0            9303            9303  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            7977           1          7977.0            7977            7977  connect                                                                         \n",
      "    0.0            7536           1          7536.0            7536            7536  pipe2                                                                           \n",
      "    0.0            2904           1          2904.0            2904            2904  bind                                                                            \n",
      "    0.0            1934           1          1934.0            1934            1934  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report19.qdrep\"\n",
      "Report file moved to \"/dli/task/report19.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](../edit/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-7a83-73b6-bbef-082c.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-7a83-73b6-bbef-082c.qdrep\"\n",
      "Exporting 1884 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-7a83-73b6-bbef-082c.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   77.2       244027021           3      81342340.3           18708       243940587  cudaMallocManaged                                                               \n",
      "   16.2        51154553           1      51154553.0        51154553        51154553  cudaDeviceSynchronize                                                           \n",
      "    6.5        20694622           3       6898207.3         6056341         8403875  cudaFree                                                                        \n",
      "    0.0           75225           4         18806.3            5551           53013  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "   96.6        49437485           3      16479161.7        15768800        17786735  initWith(float, float*, int)                                                                                                                                                                                                                                                                                                                 \n",
      "    3.4         1715383           1       1715383.0         1715383         1715383  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21332416         768         27776.6            1632          172992  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   50.2       773612994          43      17990999.9           24358       100143237  poll                                                                            \n",
      "   42.9       661846181          42      15758242.4           14444       100968972  sem_timedwait                                                                   \n",
      "    5.2        79714945         655        121702.2            1004        17651423  ioctl                                                                           \n",
      "    1.5        23061825          91        253426.6            1306         8294700  mmap                                                                            \n",
      "    0.1         1869742          77         24282.4            6885           40309  open64                                                                          \n",
      "    0.0          206854           4         51713.5           38092           56739  pthread_create                                                                  \n",
      "    0.0          193095           3         64365.0           53623           80508  fgets                                                                           \n",
      "    0.0          147640          25          5905.6            1498           28330  fopen                                                                           \n",
      "    0.0          108434           7         15490.6            8306           29235  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           99825          11          9075.0            4414           13779  write                                                                           \n",
      "    0.0           71768          16          4485.5            1694            9087  munmap                                                                          \n",
      "    0.0           43386           5          8677.2            5976           11274  open                                                                            \n",
      "    0.0           31075          18          1726.4            1086            5904  fclose                                                                          \n",
      "    0.0           26503          17          1559.0            1008            7159  fcntl                                                                           \n",
      "    0.0           25859          12          2154.9            1051            3943  read                                                                            \n",
      "    0.0           19920           2          9960.0            8104           11816  socket                                                                          \n",
      "    0.0           13011           3          4337.0            2576            5531  fread                                                                           \n",
      "    0.0           10332           4          2583.0            2032            3480  mprotect                                                                        \n",
      "    0.0            9366           1          9366.0            9366            9366  connect                                                                         \n",
      "    0.0            8120           1          8120.0            8120            8120  pipe2                                                                           \n",
      "    0.0            6685           1          6685.0            6685            6685  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            3374           1          3374.0            3374            3374  bind                                                                            \n",
      "    0.0            2330           1          2330.0            2330            2330  sem_wait                                                                        \n",
      "    0.0            1810           1          1810.0            1810            1810  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report20.qdrep\"\n",
      "Report file moved to \"/dli/task/report20.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](../edit/09-saxpy/01-saxpy.cu). It currently contains a couple of bugs that you will need to find and fix before you can successfully compile, run, and then profile it with `nsys profile`.\n",
    "\n",
    "After fixing the bugs and profiling the application, record the runtime of the `saxpy` kernel and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *300us*. Check out [the solution](../edit/09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \r\n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-1845-358d-ff55-fec3.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-1845-358d-ff55-fec3.qdrep\"\n",
      "Exporting 1095 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-1845-358d-ff55-fec3.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   94.7       239161652           3      79720550.7           30592       239088925  cudaMallocManaged                                                               \n",
      "    4.6        11723366           3       3907788.7          850049         9782034  cudaFree                                                                        \n",
      "    0.6         1546498           3        515499.3            8736         1412148  cudaMemPrefetchAsync                                                            \n",
      "    0.0           44469           1         44469.0           44469           44469  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0           33759           1         33759.0           33759           33759  saxpy(int*, int*, int*)                                                                                                                                                                                                                                                                                                                      \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0         8248544          24        343689.3          340000          351840  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "          49152.000              24             2048.000           2048.000             2048.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   45.1       373832906          20      18691645.3           21732       100067410  sem_timedwait                                                                   \n",
      "   41.8       346487756          24      14436989.8           25189       100135146  poll                                                                            \n",
      "   11.8        98020632         657        149194.3            1027        17883799  ioctl                                                                           \n",
      "    0.7         5683688          91         62458.1            1306         1263528  mmap                                                                            \n",
      "    0.4         3089926           3       1029975.3           34295         1758358  sem_wait                                                                        \n",
      "    0.2         1654576          77         21488.0            4632           42551  open64                                                                          \n",
      "    0.0          260770           5         52154.0           40083           58162  pthread_create                                                                  \n",
      "    0.0          162249           3         54083.0           51702           58319  fgets                                                                           \n",
      "    0.0          153592          25          6143.7            1497           28294  fopen                                                                           \n",
      "    0.0          139330          13         10717.7            4251           37332  write                                                                           \n",
      "    0.0           79609           5         15921.8           11373           18018  pthread_rwlock_timedwrlock                                                      \n",
      "    0.0           46904          12          3908.7            2101            7136  munmap                                                                          \n",
      "    0.0           39214           5          7842.8            3281           11223  open                                                                            \n",
      "    0.0           34533          15          2302.2            1298            4315  read                                                                            \n",
      "    0.0           28305          18          1572.5            1106            4273  fclose                                                                          \n",
      "    0.0           20263          14          1447.4            1024            5031  fcntl                                                                           \n",
      "    0.0           18579           2          9289.5            8070           10509  socket                                                                          \n",
      "    0.0           18363           2          9181.5            7273           11090  fread                                                                           \n",
      "    0.0           11557           5          2311.4            1979            2511  mprotect                                                                        \n",
      "    0.0            7910           1          7910.0            7910            7910  connect                                                                         \n",
      "    0.0            7354           1          7354.0            7354            7354  pipe2                                                                           \n",
      "    0.0            5951           1          5951.0            5951            5951  pthread_rwlock_timedrdlock                                                      \n",
      "    0.0            2747           1          2747.0            2747            2747  bind                                                                            \n",
      "    0.0            1699           1          1699.0            1699            1699  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report21.qdrep\"\n",
      "Report file moved to \"/dli/task/report21.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
